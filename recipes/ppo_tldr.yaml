dataset_name: trl-internal-testing/tldr-preference-sft-trl-style
model_name_or_path: google/gemma-2-2b-it
per_device_train_batch_size: 1
per_device_eval_batch_size: 2
learning_rate: 3e-6
gradient_accumulation_steps: 64
total_episodes: 30000
logging_steps: 10
eval_strategy: steps
eval_steps: 200
save_strategy: steps
save_steps: 400
reward_model_path: cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr
missing_eos_penalty: 1.0
stop_token: eos
response_length: 53
run_name: gemma-2-2b-it-tldr-ppo
output_dir: outputs/gemma-2-2b-it-tldr-ppo
report_to: wandb
bf16: True
dataset_num_proc: 32

use_peft: True
lora_r: 32
lora_alpha: 128
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj